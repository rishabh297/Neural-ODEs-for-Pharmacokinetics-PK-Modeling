{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4ca4d8b4",
   "metadata": {},
   "source": [
    "# Code Annotations/Analysis of Model + Training/Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "05b844ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "# from torch.nn.modules.rnn import GRU, LSTM, RNN\n",
    "import utils\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from random import SystemRandom\n",
    "from tqdm import tqdm\n",
    "\n",
    "from args import args\n",
    "import torch.optim as optim\n",
    "from torchdiffeq import odeint_adjoint as odeint\n",
    "\n",
    "from data_parse import parse_tdm1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d311e48d",
   "metadata": {},
   "source": [
    "## model.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "052fa25a",
   "metadata": {},
   "source": [
    "The encoder class defines an encoder network following variational autoencoder concept. This means that the inputs are mapped to a distribution rather than a deterministic outcome. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5912a403",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "\n",
    "    #initializes attributes of instances of encoder\n",
    "    def __init__(self, input_dim, output_dim, hidden_dim, device=torch.device(\"cpu\")):\n",
    "        super(Encoder, self).__init__()\n",
    "\n",
    "        self.output_dim = output_dim\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.device = device\n",
    "\n",
    "    #Sets up a sequential layers for network. Encoders analyzes a single element of the input sequence, \"retains/encodes\" important info about that element, and propogates forward. \n",
    "        self.hiddens_to_output = nn.Sequential(\n",
    "            nn.Linear(self.hidden_dim, self.hidden_dim),\n",
    "            #ReLU activation function is similar to SELU except it takes on binary values and can result in dead neurons, causing them to not be used for predicing outputs from features.\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(self.hidden_dim, self.output_dim),\n",
    "        )\n",
    "        #utils function that serves similar purpose as for loop in ODEFunc class. However, this function initializes biases as a 0 constant while weights are sampled from gaussian dist.\n",
    "        utils.init_network_weights(self.hiddens_to_output, std=0.001)\n",
    "\n",
    "        #nn.GRU applies a \"pre-built\" GRU to a given input; the GRU \"scans\" through the time series data in reverse and encodes the relevant data into a 12-element array. This array is fed into the ODEFunc network to define the mean and standard deviation of the latent state distributions which z_t0 is sampled from. \n",
    "        # self.rnn = nn.RNN(self.input_dim, self.hidden_dim, nonlinearity=\"relu\").to(device)\n",
    "        self.rnn = nn.GRU(self.input_dim, self.hidden_dim).to(device)\n",
    "\n",
    "    #defines forward pass of encoder\n",
    "    def forward(self, data):\n",
    "        #permutes data to make necessary dimensional changes\n",
    "        data = data.permute(1, 0, 2)\n",
    "        #reverses data to allow GRU to scan through time series data in reverse fashion (why?)\n",
    "        data = utils.reverse(data)\n",
    "        #sends input data through GRU\n",
    "        output_rnn, _ = self.rnn(data)\n",
    "        #print(output_rnn)\n",
    "        #takes in the data scanned in reverse (done by GRU) and feeds through \n",
    "        outputs = self.hiddens_to_output(output_rnn[-1])\n",
    "        #print(outputs)\n",
    "        \n",
    "        return outputs\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06f4153e",
   "metadata": {},
   "source": [
    "The ODEFunc class is a neural network responsible for uncovering the underlying differential equation for the dyanmical system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f9421a7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ODEFunc(nn.Module):\n",
    "\n",
    "    #initializes neural network with desired dimensions\n",
    "    def __init__(self, input_dim, hidden_dim):\n",
    "        super(ODEFunc, self).__init__()\n",
    "\n",
    "        #nn.Sequential is a method that allows the creation of layers in the neural network. The method itself acts as a \"container\" for the layers/modules inside the network.\n",
    "        self.net = nn.Sequential(\n",
    "            #layers in a neural network are nothing but a series of linear transformations on our input matrix (y = xAt + b). nn.Linear forms a \"linear layer\" which applies learnable weights (x) and biases (b) to our input data (At). The dimensionality of data often changes hence the allowance of input_dim and hidden-dim. \n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            #nn.SeLu is an activation function. Activation functions determine the weighted \"importance\" of features in the input data. This adds non-linearity to our model which allows it to be more complex than simple linear regression. SELU specifically allows for self-normalizing neural nets and tackles the vanishing gradient problem.\n",
    "            nn.SELU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.SELU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.SELU(),\n",
    "            nn.Linear(hidden_dim, input_dim)\n",
    "        )\n",
    "\n",
    "        # this for loop interates through every linear layer (set of layers is returned by calling self.net.modulesi) in the neural network we defined above and initializes weights/biases of input feature\n",
    "        for m in self.net.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                #if a module (layer) in our network is a linear layer and not an activation function, this randomly initializes our input tensor of weights of each layer (m.weights) with values sampled from a Gaussian distribution with mean 0 and SD 0.001. This mitigates the vanishing/exploding gradient problem.\n",
    "                nn.init.normal_(m.weight, mean=0, std=0.001)\n",
    "                # if a module is a linear layer, then the input tensor (tensor containing biases of the layer) are all initialized to 0.5\n",
    "                nn.init.constant_(m.bias, val=0.5)\n",
    "\n",
    "    #feeds our input data through our network (self.net)\n",
    "    def forward(self, t, x):\n",
    "        # print(x)\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f04c227b",
   "metadata": {},
   "source": [
    "Classifier initializes and defines a decoder network that takes in the sequence of z_t's outputted by the ODEFunc network. It then generates the predictions from the output of the ODE solver and the first dosing observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "484c3dc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#initializes and defines a decoder network that takes in the sequence of z_t's outputted by the ODEFunc network. It then generates the predictions from the output of the ODE solver and the first dosing observations.\n",
    "class Classifier(nn.Module):\n",
    "\n",
    "    #init method creates another set of sequential modules with 1 fully connected layer and 32 hidden units\n",
    "    def __init__(self, latent_dim, output_dim):\n",
    "        super(Classifier, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(latent_dim + 20, 32),\n",
    "            nn.SELU(),\n",
    "            nn.Linear(32, output_dim)\n",
    "        )\n",
    "        \n",
    "        #follows same weight and bias initialization protocol as the Encoder class\n",
    "        utils.init_network_weights(self.net, std=0.001)\n",
    "\n",
    "    #defines forward pass where z is the sequence of z_t's generated by the output of ODEFunc and cmax_time refers to the dosing information.\n",
    "    def forward(self, z, cmax_time):\n",
    "        #repeates dosing information along given dimensions to match up with z\n",
    "        cmax_time = cmax_time.repeat(z.size(0), 1, 1)\n",
    "        #joins dosing info with sequence of z_t's and feeds in as input to decoder\n",
    "        z = torch.cat([z, cmax_time], 2)\n",
    "        return self.net(z)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "944c0454",
   "metadata": {},
   "source": [
    "## run_train.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "39f7d6bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "tdm1_obj = parse_tdm1(device, phase=\"train\")\n",
    "input_dim = tdm1_obj[\"input_dim\"]\n",
    "hidden_dim = 128\n",
    "latent_dim = 6\n",
    "\n",
    "encoder = Encoder(input_dim=input_dim, output_dim=2 * latent_dim, hidden_dim=hidden_dim)\n",
    "ode_func = ODEFunc(input_dim=latent_dim, hidden_dim=16)\n",
    "classifier = Classifier(latent_dim=latent_dim, output_dim=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cbb8603",
   "metadata": {},
   "source": [
    "tdm1_obj is a dictionary that contains a generator that outputs every \"data point\" when next( ) is called"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "79fa0e34",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train_dataloader': <generator object inf_generator at 0x7f77cbd01ba0>,\n",
       " 'val_dataloader': <generator object inf_generator at 0x7f77cbd01c10>,\n",
       " 'n_train_batches': 656,\n",
       " 'n_val_batches': 56,\n",
       " 'input_dim': 5}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tdm1_obj"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0331cf3f",
   "metadata": {},
   "source": [
    "Each next( ) returns a patient data point where each tensor contains the relevant information for each feature per patient\n",
    "- TFDS (time in hours between each dose)\n",
    "- AMT (dosing amount in milligrams)\n",
    "- TIME (time in hours since treatment started)\n",
    "- CYCL (current dosing cycle #)\n",
    "- PK (pk info for first cycle))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "dfdc8005",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([120.2],\n",
       " tensor([0., 1., 2., 3., 5.]),\n",
       " tensor([[[  0.0000,   0.0000,   1.0000, 136.0000,   1.0000],\n",
       "          [ 24.0000,   1.0000,   1.0000,   0.0000,   0.9453],\n",
       "          [ 48.0000,   2.0000,   1.0000,   0.0000,   0.6941],\n",
       "          [ 72.0000,   3.0000,   1.0000,   0.0000,   0.5371],\n",
       "          [120.0000,   5.0000,   1.0000,   0.0000,   0.3670]]]),\n",
       " tensor([[[28.3290],\n",
       "          [26.7800],\n",
       "          [19.6640],\n",
       "          [15.2150],\n",
       "          [10.3980]]]),\n",
       " tensor([[ 0.0000, 28.3290,  1.0000, 26.7800,  2.0000, 19.6640,  3.0000, 15.2150,\n",
       "           5.0000, 10.3980,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "           0.0000,  0.0000,  0.0000,  0.0000]]))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(tdm1_obj[\"train_dataloader\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "57dc0d06",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Encoder(\n",
       "  (hiddens_to_output): Sequential(\n",
       "    (0): Linear(in_features=128, out_features=128, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=128, out_features=12, bias=True)\n",
       "  )\n",
       "  (rnn): GRU(5, 128)\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45988c83",
   "metadata": {},
   "source": [
    "Encoder Structure:\n",
    "- Data is fed through GRU to reversely scan the data\n",
    "    - GRU input dim: 5\n",
    "    - GRU output dim:128\n",
    "- Why is it fed through GRU?: Based on sample codes from Chen et al., the GRU is inferring the parameters of the governing ODE using a-posteriori likelihood estimation\n",
    "- Output of GRU is then fed through 2 linear layers \n",
    "- these two linear layers fall under \"hiddens_to_output\" which takes the given info from GRU and \"condenses\" it into a 12 element vector to be passed into ODEFunc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1b2900dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ODEFunc(\n",
       "  (net): Sequential(\n",
       "    (0): Linear(in_features=6, out_features=16, bias=True)\n",
       "    (1): SELU()\n",
       "    (2): Linear(in_features=16, out_features=16, bias=True)\n",
       "    (3): SELU()\n",
       "    (4): Linear(in_features=16, out_features=16, bias=True)\n",
       "    (5): SELU()\n",
       "    (6): Linear(in_features=16, out_features=6, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ode_func"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ac6d20c",
   "metadata": {},
   "source": [
    "ODE "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "36f8385c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Classifier(\n",
       "  (net): Sequential(\n",
       "    (0): Linear(in_features=26, out_features=32, bias=True)\n",
       "    (1): SELU()\n",
       "    (2): Linear(in_features=32, out_features=1, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c91d9595",
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch size defines how many training samples must be done before updating weights/biases of a node during backprop\n",
    "#epoch defines how many total backward passes we will do\n",
    "batches_per_epoch = tdm1_obj[\"n_train_batches\"]\n",
    "#sets L2 norm-squared (MSE) between predicted and actual as loss criterion\n",
    "criterion = nn.MSELoss().to(device=device)\n",
    "params = (list(encoder.parameters()) + \n",
    "          list(ode_func.parameters()) + \n",
    "          list(classifier.parameters()))\n",
    "#utilizing Adam optimization algorithm rather than SGD to overcome saddlepoints in data. It incorporates the idea of momentum by nudging weights/biases by the average running gradient rather than the gradient itself.\n",
    "optimizer = optim.Adam(params, lr=args.lr, weight_decay=args.l2)\n",
    "best_rmse = 0x7fffffff\n",
    "best_epochs = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a6b2d389",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(data='data.csv', fold=5, model=None, save=None, continue_train=False, random_seed=1000, layer=2, lr=5e-05, l2=0.1, hidden=None, tol=0.0001, epochs=30)\n"
     ]
    }
   ],
   "source": [
    "print(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "80046ecc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|#########################################| 656/656 [01:48<00:00,  6.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "idx: tensor([[[True],\n",
      "         [True],\n",
      "         [True],\n",
      "         [True],\n",
      "         [True]]])\n",
      "Preds: tensor([11.5927, 17.4031, 17.3442, 16.8077, 16.5658], grad_fn=<IndexBackward0>)\n",
      "Labels: tensor([ 3.1167, 43.5340, 36.0590,  6.6201,  3.1166])\n",
      "rmse: 19.694857\n",
      "train_loss: 19.131277\n",
      "val_loss: 19.694857\n"
     ]
    }
   ],
   "source": [
    "#ran 1 epoch for testing purposes\n",
    "for epoch in range(1):\n",
    "\n",
    "    for _ in tqdm(range(batches_per_epoch), ascii=True):\n",
    "        #sets gradients of all parameters to zero. This prevents the incorrect accumulation of gradients that occurs if you call loss.backwards more than once wihtout zeroing out the gradients.\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        #extracts training features and dosing\n",
    "        ptnms, times, features, labels, cmax_time = tdm1_obj[\"train_dataloader\"].__next__()\n",
    "        dosing = torch.zeros([features.size(0), features.size(1), latent_dim])\n",
    "        dosing[:, :, 0] = features[:, :, -2]\n",
    "        dosing = dosing.permute(1, 0, 2)\n",
    "\n",
    "        #VAE concept used here. We are taking the output of the encoder and sampling z_0 from a distribution of the latent space variables gathered from estimating the mean and variance from the 12 elements outputted by the encoder. \n",
    "        encoder_out = encoder(features)\n",
    "        qz0_mean, qz0_var = encoder_out[:, :latent_dim], encoder_out[:, latent_dim:]\n",
    "        z0 = utils.sample_standard_gaussian(qz0_mean, qz0_var)\n",
    "        \n",
    "        solves = z0.unsqueeze(0).clone()\n",
    "        try:\n",
    "            #this is where the idea of neural-ODE's are used. dosing information and time interval are incorporated into the event from the previous time step. The time interval from the previous time interval and z_i-1 are sent into the ODE Solver function.\n",
    "            for idx, (time0, time1) in enumerate(zip(times[:-1], times[1:])):\n",
    "                z0 += dosing[idx]\n",
    "                time_interval = torch.Tensor([time0 - time0, time1 - time0])\n",
    "                #ODE Solver function \n",
    "                sol = odeint(ode_func, z0, time_interval, rtol=1e-4, atol=1e-4)\n",
    "                z0 = sol[-1].clone()\n",
    "                #running sequence of all z_i's at each time step which will eventially be used to predict output in the decoder\n",
    "                solves = torch.cat([solves, sol[-1:, :]], 0)\n",
    "        except AssertionError:\n",
    "            print(times)\n",
    "            print(time0, time1, time_interval, ptnms)\n",
    "            continue\n",
    "\n",
    "        #prediction generation from sequence of z_i's\n",
    "        preds = classifier(solves, cmax_time)\n",
    "\n",
    "        # computes MSE on preds vs observations \n",
    "        loss = utils.compute_loss_on_train(criterion, labels, preds)\n",
    "        try: \n",
    "            #automatically computes gradients of loss tensor\n",
    "            loss.backward()\n",
    "        except RuntimeError:\n",
    "            print(ptnms)\n",
    "            print(times)\n",
    "            continue\n",
    "        #performs a single parameter update (single optimization step)\n",
    "        optimizer.step()\n",
    "    \n",
    "    idx_not_nan = ~(torch.isnan(labels) | (labels == -1))\n",
    "    print(\"idx: \" + str(idx_not_nan))\n",
    "    preds = preds.permute(1, 0, 2)[idx_not_nan]\n",
    "    labels = labels[idx_not_nan]\n",
    "    print(\"Preds: \" + str(preds))\n",
    "    print(\"Labels: \" + str(labels))\n",
    "\n",
    "    #torch.no_grad() is used to prevent the automatic calculation of gradients to clearly see unbiased training/validation error \n",
    "    with torch.no_grad():\n",
    "        \n",
    "        #training error\n",
    "        train_res = utils.compute_loss_on_test(encoder, ode_func, classifier, args,\n",
    "            tdm1_obj[\"train_dataloader\"], tdm1_obj[\"n_train_batches\"], \n",
    "            device, phase=\"train\")\n",
    "\n",
    "        #validation error\n",
    "        validation_res = utils.compute_loss_on_test(encoder, ode_func, classifier, args,\n",
    "            tdm1_obj[\"val_dataloader\"], tdm1_obj[\"n_val_batches\"], \n",
    "            device, phase=\"validate\")\n",
    "        \n",
    "        train_loss = train_res[\"loss\"] \n",
    "        validation_loss = validation_res[\"loss\"]\n",
    "\n",
    "        #if the validation loss on the current interation is better than the best running RMSE, then we save the weights and biases of the encoder, ode, classifier, and arguments\n",
    "        #if validation_loss < best_rmse:\n",
    "         #   torch.save({'encoder': encoder.state_dict(),\n",
    "          #              'ode': ode_func.state_dict(),\n",
    "           #             'classifier': classifier.state_dict(),\n",
    "            #            'args': args}, ckpt_path)\n",
    "        best_rmse = validation_loss\n",
    "        best_epochs = epoch\n",
    "\n",
    "print(\"rmse: \" + str(best_rmse))\n",
    "print(\"train_loss: \" + str(train_loss))\n",
    "print(\"val_loss: \" + str(validation_loss))\n",
    "        #message = \"\"\"\n",
    "        #Epoch {:04d} | Training loss {:.6f} | Training R2 {:.6f} | Validation loss {:.6f} | Validation R2 {:.6f}\n",
    "        #Best loss {:.6f} | Best epoch {:04d}\n",
    "        #\"\"\".format(epoch, train_loss, train_res[\"r2\"], validation_loss, validation_res[\"r2\"], best_rmse, best_epochs)\n",
    "        #logger.info(message)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81e36a4b",
   "metadata": {},
   "source": [
    "## run_predict.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "6f96513f",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ckpt_path' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/d3/_4zxml3949n9w23hrq3ngqrm0000gn/T/ipykernel_82643/3863958866.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;31m#loads the model's parameter dictionary\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mckpt_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mode_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclassifier\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;31m########################################################################\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'ckpt_path' is not defined"
     ]
    }
   ],
   "source": [
    "########################################################################\n",
    "#parses input data into feature columns, etc.\n",
    "tdm1_obj = parse_tdm1(device, phase=\"test\")\n",
    "input_dim = tdm1_obj[\"input_dim\"]\n",
    "#represents hidden units of GRU in encoder\n",
    "hidden_dim = 128 \n",
    "latent_dim = 6\n",
    "\n",
    "#instantiates encoder. Output dimension is 12 because 6 elements are used to determine the value of the mean for the distribution of the latent space while the other 6 are used to estimate the variance.\n",
    "encoder = Encoder(input_dim=input_dim, output_dim=2 * latent_dim, hidden_dim=hidden_dim)\n",
    "#instantiates governing ODEFunc\n",
    "ode_func = ODEFunc(input_dim=latent_dim, hidden_dim=16)\n",
    "#instantiates decoder\n",
    "classifier = Classifier(latent_dim=latent_dim, output_dim=1)\n",
    "\n",
    "#loads the model's parameter dictionary\n",
    "utils.load_model(ckpt_path, encoder, ode_func, classifier, device)\n",
    "\n",
    "########################################################################\n",
    "## Predict & Evaluate\n",
    "#disables gradient calculation, allowing for less memory consumption and faster compute. It is generally used to perform validation/testing because gradients are not required to be computed when testing model performance.\n",
    "with torch.no_grad():\n",
    "    #uses compute loss on test\n",
    "    #the function compute_loss_ is where the ODE solver functions integrate the dosing info and time interval. This is also where the concept of VAE's are used where z_0 is sampled from the latent distribution (which is derived from the mean and variance calculated by the 12 element input array). see page 6 of paper for more specific info. \n",
    "    test_res = utils.compute_loss_on_test(encoder, ode_func, classifier, args,\n",
    "        tdm1_obj[\"test_dataloader\"], tdm1_obj[\"n_test_batches\"], \n",
    "        device, phase=\"test\")\n",
    "\n",
    "eval_results = pd.DataFrame(test_res).drop(columns=\"loss\")\n",
    "eval_results.to_csv(eval_path, index=False)\n",
    "\n",
    "with torch.no_grad():\n",
    "    #uses compute loss on interpolated data. Interpolated data contains estimated \"intermediate\" values between data points to smooth out the data\n",
    "    test_res = utils.compute_loss_on_interp(encoder, ode_func, classifier, args,\n",
    "        tdm1_obj[\"interp\"], tdm1_obj[\"test_dataloader\"], tdm1_obj[\"n_test_batches\"], \n",
    "        device, phase=\"test\")\n",
    "\n",
    "#puts results in a data frame and migrates to csv file\n",
    "eval_results = pd.DataFrame(test_res).drop(columns=\"loss\")\n",
    "eval_results.to_csv(eval_path + \".interp\", index=False)\n",
    "\n",
    "with torch.no_grad():\n",
    "    #uses compute loss on interpolated data without dosing info\n",
    "    test_res = utils.compute_loss_on_interp(encoder, ode_func, classifier, args,\n",
    "        tdm1_obj[\"nodosing\"], tdm1_obj[\"test_dataloader\"], tdm1_obj[\"n_test_batches\"], \n",
    "        device, phase=\"test\")\n",
    "\n",
    "eval_results = pd.DataFrame(test_res).drop(columns=\"loss\")\n",
    "eval_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3e55cc0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
