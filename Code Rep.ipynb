{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "05b844ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "# from torch.nn.modules.rnn import GRU, LSTM, RNN\n",
    "import utils\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from random import SystemRandom\n",
    "from tqdm import tqdm\n",
    "\n",
    "from args import args\n",
    "import torch.optim as optim\n",
    "from torchdiffeq import odeint_adjoint as odeint\n",
    "\n",
    "from data_parse import parse_tdm1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "f9421a7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ODEFunc(nn.Module):\n",
    "\n",
    "    #initializes neural network with desired dimensions\n",
    "    def __init__(self, input_dim, hidden_dim):\n",
    "        super(ODEFunc, self).__init__()\n",
    "\n",
    "        #nn.Sequential is a method that allows the creation of layers in the neural network. The method itself acts as a \"container\" for the layers/modules inside the network.\n",
    "        self.net = nn.Sequential(\n",
    "            #layers in a neural network are nothing but a series of linear transformations on our input matrix (y = xAt + b). nn.Linear forms a \"linear layer\" which applies learnable weights (x) and biases (b) to our input data (At). The dimensionality of data often changes hence the allowance of input_dim and hidden-dim. \n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            #nn.SeLu is an activation function. Activation functions determine the weighted \"importance\" of features in the input data. This adds non-linearity to our model which allows it to be more complex than simple linear regression. SELU specifically allows for self-normalizing neural nets and tackles the vanishing gradient problem.\n",
    "            nn.SELU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.SELU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.SELU(),\n",
    "            nn.Linear(hidden_dim, input_dim)\n",
    "        )\n",
    "\n",
    "        # this for loop interates through every linear layer (set of layers is returned by calling self.net.modulesi) in the neural network we defined above and initializes weights/biases of input feature\n",
    "        for m in self.net.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                #if a module (layer) in our network is a linear layer and not an activation function, this randomly initializes our input tensor of weights of each layer (m.weights) with values sampled from a Gaussian distribution with mean 0 and SD 0.001. This mitigates the vanishing/exploding gradient problem.\n",
    "                nn.init.normal_(m.weight, mean=0, std=0.001)\n",
    "                # if a module is a linear layer, then the input tensor (tensor containing biases of the layer) are all initialized to 0.5\n",
    "                nn.init.constant_(m.bias, val=0.5)\n",
    "\n",
    "    #feeds our input data through our network (self.net)\n",
    "    def forward(self, t, x):\n",
    "        # print(x)\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "5912a403",
   "metadata": {},
   "outputs": [],
   "source": [
    "#defines an encoder network following variational autoencoder concept; this means that the inputs are mapped to a distribution rather than a deterministic outcome. \n",
    "class Encoder(nn.Module):\n",
    "\n",
    "    #initializes attributes of instances of encoder\n",
    "    def __init__(self, input_dim, output_dim, hidden_dim, device=torch.device(\"cpu\")):\n",
    "        super(Encoder, self).__init__()\n",
    "\n",
    "        self.output_dim = output_dim\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.device = device\n",
    "\n",
    "    #Sets up a sequential layers for network. Encoders analyzes a single element of the input sequence, \"retains/encodes\" important info about that element, and propogates forward. \n",
    "        self.hiddens_to_output = nn.Sequential(\n",
    "            nn.Linear(self.hidden_dim, self.hidden_dim),\n",
    "            #ReLU activation function is similar to SELU except it takes on binary values and can result in dead neurons, causing them to not be used for predicing outputs from features.\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(self.hidden_dim, self.output_dim),\n",
    "        )\n",
    "        #utils function that serves similar purpose as for loop in ODEFunc class. However, this function initializes biases as a 0 constant while weights are sampled from gaussian dist.\n",
    "        utils.init_network_weights(self.hiddens_to_output, std=0.001)\n",
    "\n",
    "        #nn.GRU applies a \"pre-built\" GRU to a given input; the GRU \"scans\" through the time series data in reverse and encodes the relevant data into a 12-element array. This array is fed into the ODEFunc network to define the mean and standard deviation of the latent state distributions which z_t0 is sampled from. \n",
    "        # self.rnn = nn.RNN(self.input_dim, self.hidden_dim, nonlinearity=\"relu\").to(device)\n",
    "        self.rnn = nn.GRU(self.input_dim, self.hidden_dim).to(device)\n",
    "\n",
    "    #defines forward pass of encoder\n",
    "    def forward(self, data):\n",
    "        #permutes data to make necessary dimensional changes\n",
    "        data = data.permute(1, 0, 2)\n",
    "        #reverses data to allow GRU to scan through time series data in reverse fashion (why?)\n",
    "        data = utils.reverse(data)\n",
    "        #sends input data through GRU\n",
    "        output_rnn, _ = self.rnn(data)\n",
    "        #print(output_rnn)\n",
    "        #takes in the data scanned in reverse (done by GRU) and feeds through \n",
    "        outputs = self.hiddens_to_output(output_rnn[-1])\n",
    "        #print(outputs)\n",
    "        \n",
    "        return outputs\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "484c3dc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#initializes and defines a decoder network that takes in the sequence of z_t's outputted by the ODEFunc network. It then generates the predictions from the output of the ODE solver and the first dosing observations.\n",
    "class Classifier(nn.Module):\n",
    "\n",
    "    #init method creates another set of sequential modules with 1 fully connected layer and 32 hidden units\n",
    "    def __init__(self, latent_dim, output_dim):\n",
    "        super(Classifier, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(latent_dim + 20, 32),\n",
    "            nn.SELU(),\n",
    "            nn.Linear(32, output_dim)\n",
    "        )\n",
    "        \n",
    "        #follows same weight and bias initialization protocol as the Encoder class\n",
    "        utils.init_network_weights(self.net, std=0.001)\n",
    "\n",
    "    #defines forward pass where z is the sequence of z_t's generated by the output of ODEFunc and cmax_time refers to the dosing information.\n",
    "    def forward(self, z, cmax_time):\n",
    "        #repeates dosing information along given dimensions to match up with z\n",
    "        cmax_time = cmax_time.repeat(z.size(0), 1, 1)\n",
    "        #joins dosing info with sequence of z_t's and feeds in as input to decoder\n",
    "        z = torch.cat([z, cmax_time], 2)\n",
    "        return self.net(z)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "944c0454",
   "metadata": {},
   "source": [
    "The following code is referring to run_train.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "39f7d6bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train_dataloader': <generator object inf_generator at 0x7f853572c580>,\n",
       " 'val_dataloader': <generator object inf_generator at 0x7f853572ccf0>,\n",
       " 'n_train_batches': 656,\n",
       " 'n_val_batches': 56,\n",
       " 'input_dim': 5}"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "tdm1_obj = parse_tdm1(device, phase=\"train\")\n",
    "input_dim = tdm1_obj[\"input_dim\"]\n",
    "hidden_dim = 128\n",
    "latent_dim = 6\n",
    "\n",
    "encoder = Encoder(input_dim=input_dim, output_dim=2 * latent_dim, hidden_dim=hidden_dim)\n",
    "ode_func = ODEFunc(input_dim=latent_dim, hidden_dim=16)\n",
    "classifier = Classifier(latent_dim=latent_dim, output_dim=1)\n",
    "tdm1_obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "1018a33a",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = Encoder(input_dim=input_dim, output_dim=2 * latent_dim, hidden_dim=hidden_dim)\n",
    "ode_func = ODEFunc(input_dim=latent_dim, hidden_dim=16)\n",
    "classifier = Classifier(latent_dim=latent_dim, output_dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "c91d9595",
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch size defines how many training samples must be done before updating weights/biases of a node during backprop\n",
    "#epoch defines how many total backward passes we will do\n",
    "batches_per_epoch = tdm1_obj[\"n_train_batches\"]\n",
    "#sets L2 norm-squared (MSE) between predicted and actual as loss criterion\n",
    "criterion = nn.MSELoss().to(device=device)\n",
    "params = (list(encoder.parameters()) + \n",
    "          list(ode_func.parameters()) + \n",
    "          list(classifier.parameters()))\n",
    "#utilizing Adam optimization algorithm rather than SGD to overcome saddlepoints in data. It incorporates the idea of momentum by nudging weights/biases by the average running gradient rather than the gradient itself.\n",
    "#optimizer = optim.Adam(params, lr=args.lr, weight_decay=args.l2)\n",
    "best_rmse = 0x7fffffff\n",
    "best_epochs = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "a6b2d389",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(data=None, fold=None, model=None, save=None, continue_train=False, random_seed=1000, layer=2, lr=None, l2=None, hidden=None, tol=None, epochs=None)\n"
     ]
    }
   ],
   "source": [
    "print(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "80046ecc",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object cannot be interpreted as an integer",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/d3/_4zxml3949n9w23hrq3ngqrm0000gn/T/ipykernel_81531/2916194992.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatches_per_epoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mascii\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0;31m#sets gradients of all parameters to zero. This prevents the incorrect accumulation of gradients that occurs if you call loss.backwards more than once wihtout zeroing out the gradients.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'NoneType' object cannot be interpreted as an integer"
     ]
    }
   ],
   "source": [
    "for epoch in range(1, args.epochs):\n",
    "\n",
    "    for _ in tqdm(range(batches_per_epoch), ascii=True):\n",
    "        #sets gradients of all parameters to zero. This prevents the incorrect accumulation of gradients that occurs if you call loss.backwards more than once wihtout zeroing out the gradients.\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        #extracts training features and dosing\n",
    "        ptnms, times, features, labels, cmax_time = tdm1_obj[\"train_dataloader\"].__next__()\n",
    "        dosing = torch.zeros([features.size(0), features.size(1), latent_dim])\n",
    "        dosing[:, :, 0] = features[:, :, -2]\n",
    "        dosing = dosing.permute(1, 0, 2)\n",
    "\n",
    "        #VAE concept used here. We are taking the output of the encoder and sampling z_0 from a distribution of the latent space variables gathered from estimating the mean and variance from the 12 elements outputted by the encoder. \n",
    "        encoder_out = encoder(features)\n",
    "        qz0_mean, qz0_var = encoder_out[:, :latent_dim], encoder_out[:, latent_dim:]\n",
    "        z0 = utils.sample_standard_gaussian(qz0_mean, qz0_var)\n",
    "        \n",
    "        solves = z0.unsqueeze(0).clone()\n",
    "        try:\n",
    "            #this is where the idea of neural-ODE's are used. dosing information and time interval are incorporated into the event from the previous time step. The time interval from the previous time interval and z_i-1 are sent into the ODE Solver function.\n",
    "            for idx, (time0, time1) in enumerate(zip(times[:-1], times[1:])):\n",
    "                z0 += dosing[idx]\n",
    "                time_interval = torch.Tensor([time0 - time0, time1 - time0])\n",
    "                #ODE Solver function \n",
    "                sol = odeint(ode_func, z0, time_interval, rtol=args.tol, atol=args.tol)\n",
    "                z0 = sol[-1].clone()\n",
    "                #running sequence of all z_i's at each time step which will eventially be used to predict output in the decoder\n",
    "                solves = torch.cat([solves, sol[-1:, :]], 0)\n",
    "        except AssertionError:\n",
    "            print(times)\n",
    "            print(time0, time1, time_interval, ptnms)\n",
    "            continue\n",
    "\n",
    "        #prediction generation from sequence of z_i's\n",
    "        preds = classifier(solves, cmax_time)\n",
    "\n",
    "        # computes MSE on preds vs observations \n",
    "        loss = utils.compute_loss_on_train(criterion, labels, preds)\n",
    "        try: \n",
    "            #automatically computes gradients of loss tensor\n",
    "            loss.backward()\n",
    "        except RuntimeError:\n",
    "            print(ptnms)\n",
    "            print(times)\n",
    "            continue\n",
    "        #performs a single parameter update (single optimization step)\n",
    "        optimizer.step()\n",
    "    \n",
    "    idx_not_nan = ~(torch.isnan(labels) | (labels == -1))\n",
    "    preds = preds.permute(1, 0, 2)[idx_not_nan]\n",
    "    labels = labels[idx_not_nan]\n",
    "    print(preds)\n",
    "    print(labels)\n",
    "\n",
    "    #torch.no_grad() is used to prevent the automatic calculation of gradients to clearly see unbiased training/validation error \n",
    "    with torch.no_grad():\n",
    "        \n",
    "        #training error\n",
    "        train_res = utils.compute_loss_on_test(encoder, ode_func, classifier, args,\n",
    "            tdm1_obj[\"train_dataloader\"], tdm1_obj[\"n_train_batches\"], \n",
    "            device, phase=\"train\")\n",
    "\n",
    "        #validation error\n",
    "        validation_res = utils.compute_loss_on_test(encoder, ode_func, classifier, args,\n",
    "            tdm1_obj[\"val_dataloader\"], tdm1_obj[\"n_val_batches\"], \n",
    "            device, phase=\"validate\")\n",
    "        \n",
    "        train_loss = train_res[\"loss\"] \n",
    "        validation_loss = validation_res[\"loss\"]\n",
    "\n",
    "        #if the validation loss on the current interation is better than the best running RMSE, then we save the weights and biases of the encoder, ode, classifier, and arguments\n",
    "        if validation_loss < best_rmse:\n",
    "            torch.save({'encoder': encoder.state_dict(),\n",
    "                        'ode': ode_func.state_dict(),\n",
    "                        'classifier': classifier.state_dict(),\n",
    "                        'args': args}, ckpt_path)\n",
    "            best_rmse = validation_loss\n",
    "            best_epochs = epoch\n",
    "\n",
    "        message = \"\"\"\n",
    "        Epoch {:04d} | Training loss {:.6f} | Training R2 {:.6f} | Validation loss {:.6f} | Validation R2 {:.6f}\n",
    "        Best loss {:.6f} | Best epoch {:04d}\n",
    "        \"\"\".format(epoch, train_loss, train_res[\"r2\"], validation_loss, validation_res[\"r2\"], best_rmse, best_epochs)\n",
    "        logger.info(message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c796b204",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
