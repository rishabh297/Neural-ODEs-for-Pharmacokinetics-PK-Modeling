{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from random import SystemRandom\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchdiffeq import odeint_adjoint as odeint\n",
    "# from torch.nn.modules.rnn import GRU, LSTM, RNN\n",
    "\n",
    "import utils\n",
    "from args import args\n",
    "from data_parse import parse_tdm1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ODEFunc(nn.Module):\n",
    "\n",
    "    def __init__(self, input_dim, hidden_dim):\n",
    "        super(ODEFunc, self).__init__()\n",
    "\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.SELU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.SELU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.SELU(),\n",
    "            nn.Linear(hidden_dim, input_dim)\n",
    "        )\n",
    "\n",
    "        for m in self.net.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.normal_(m.weight, mean=0, std=0.001)\n",
    "                nn.init.constant_(m.bias, val=0.5)\n",
    "\n",
    "    def forward(self, t, x):\n",
    "        # print(x)\n",
    "        return self.net(x)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A neural network class that is built with 4 Linear layers and 3 SELU activation functions. Takes in time and state, and creates a neural network that function as an ODE. Runs SELU functions after each classification to keep mean the same and minimize variance to prevent compounding issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "\n",
    "    def __init__(self, input_dim, output_dim, hidden_dim, device=torch.device(\"cpu\")):\n",
    "        super(Encoder, self).__init__()\n",
    "\n",
    "        self.output_dim = output_dim\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.device = device\n",
    "\n",
    "        self.hiddens_to_output = nn.Sequential(\n",
    "            nn.Linear(self.hidden_dim, self.hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(self.hidden_dim, self.output_dim),\n",
    "        )\n",
    "        utils.init_network_weights(self.hiddens_to_output, std=0.001)\n",
    "\n",
    "        # self.rnn = nn.RNN(self.input_dim, self.hidden_dim, nonlinearity=\"relu\").to(device)\n",
    "        self.rnn = nn.GRU(self.input_dim, self.hidden_dim).to(device)\n",
    "\n",
    "    def forward(self, data):\n",
    "        data = data.permute(1, 0, 2)\n",
    "        data = utils.reverse(data)\n",
    "        output_rnn, _ = self.rnn(data)\n",
    "        #print(output_rnn)\n",
    "        outputs = self.hiddens_to_output(output_rnn[-1])\n",
    "        #print(outputs)\n",
    "        \n",
    "        return outputs"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Return a latent vector, a lower dimension version of the input data, that discards noise. Uses a GRU to this this which deals with time series data and is a type of nueral network which takes previous data and updates it. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Classifier(nn.Module):\n",
    "\n",
    "    def __init__(self, latent_dim, output_dim):\n",
    "        super(Classifier, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(latent_dim + 20, 32),\n",
    "            nn.SELU(),\n",
    "            nn.Linear(32, output_dim),\n",
    "        )\n",
    "        \n",
    "        utils.init_network_weights(self.net, std=0.001)\n",
    "\n",
    "    def forward(self, z, cmax_time):\n",
    "        cmax_time = cmax_time.repeat(z.size(0), 1, 1)\n",
    "        z = torch.cat([z, cmax_time], 2)\n",
    "        return self.net(z)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A neural network which takes in latent state from encoder and outputs a classification from it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "# file_name = os.path.basename(__file__)[:-3]\n",
    "utils.makedirs(args.save)\n",
    "\n",
    "input_cmd = sys.argv\n",
    "input_cmd = \" \".join(input_cmd)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sets up pytorch operations (either using cpu or GPU)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(args.random_seed + args.model + args.fold)\n",
    "np.random.seed(args.random_seed + args.model + args.fold)\n",
    "\n",
    "ckpt_path = os.path.join(args.save, f\"fold_{args.fold}_model_{args.model}.ckpt\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creates a seed for replication for the model which is going to be trained. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tdm1_obj = parse_tdm1(device, phase=\"train\")\n",
    "input_dim = tdm1_obj[\"input_dim\"]\n",
    "hidden_dim = 128\n",
    "latent_dim = 6\n",
    "\n",
    "encoder = Encoder(input_dim=input_dim, output_dim=2 * latent_dim, hidden_dim=hidden_dim)\n",
    "ode_func = ODEFunc(input_dim=latent_dim, hidden_dim=16)\n",
    "classifier = Classifier(latent_dim=latent_dim, output_dim=1)\n",
    "\n",
    "\n",
    "if args.continue_train:\n",
    "    utils.load_model(ckpt_path, encoder, ode_func, classifier, device)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creates instances of each of the three different previously explained neural networks. Hidden dimensions is 128 as it's a large power of 2. Latent dimension is 6 as it's number of features for the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_path = \"logs/\" + f\"fold_{args.fold}_model_{args.model}.log\"\n",
    "utils.makedirs(\"logs/\")\n",
    "logger = utils.get_logger(logpath=log_path, filepath=os.path.abspath(__file__))\n",
    "logger.info(input_cmd)\n",
    "\n",
    "batches_per_epoch = tdm1_obj[\"n_train_batches\"]\n",
    "criterion = nn.MSELoss().to(device=device)\n",
    "params = (list(encoder.parameters()) + \n",
    "          list(ode_func.parameters()) + \n",
    "          list(classifier.parameters()))\n",
    "optimizer = optim.Adam(params, lr=args.lr, weight_decay=args.l2)\n",
    "best_rmse = 0x7fffffff\n",
    "best_epochs = 0\n",
    "\n",
    "for epoch in range(1, args.epochs):\n",
    "\n",
    "    for _ in tqdm(range(batches_per_epoch), ascii=True):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        ptnms, times, features, labels, cmax_time = tdm1_obj[\"train_dataloader\"].__next__()\n",
    "        dosing = torch.zeros([features.size(0), features.size(1), latent_dim])\n",
    "        dosing[:, :, 0] = features[:, :, -2]\n",
    "        dosing = dosing.permute(1, 0, 2)\n",
    "\n",
    "        encoder_out = encoder(features)\n",
    "        qz0_mean, qz0_var = encoder_out[:, :latent_dim], encoder_out[:, latent_dim:]\n",
    "        z0 = utils.sample_standard_gaussian(qz0_mean, qz0_var)\n",
    "        \n",
    "        solves = z0.unsqueeze(0).clone()\n",
    "        try:\n",
    "            for idx, (time0, time1) in enumerate(zip(times[:-1], times[1:])):\n",
    "                z0 += dosing[idx]\n",
    "                time_interval = torch.Tensor([time0 - time0, time1 - time0])\n",
    "                sol = odeint(ode_func, z0, time_interval, rtol=args.tol, atol=args.tol)\n",
    "                z0 = sol[-1].clone()\n",
    "                solves = torch.cat([solves, sol[-1:, :]], 0)\n",
    "        except AssertionError:\n",
    "            print(times)\n",
    "            print(time0, time1, time_interval, ptnms)\n",
    "            continue\n",
    "    \n",
    "        preds = classifier(solves, cmax_time)\n",
    "\n",
    "        loss = utils.compute_loss_on_train(criterion, labels, preds)\n",
    "        try: \n",
    "            loss.backward()\n",
    "        except RuntimeError:\n",
    "            print(ptnms)\n",
    "            print(times)\n",
    "            continue\n",
    "        optimizer.step()\n",
    "    \n",
    "    idx_not_nan = ~(torch.isnan(labels) | (labels == -1))\n",
    "    preds = preds.permute(1, 0, 2)[idx_not_nan]\n",
    "    labels = labels[idx_not_nan]\n",
    "    print(preds)\n",
    "    print(labels)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creates 5 different models for each of the 5-fold training cycles. The model is trained for a specified number of epochs using an Adam optimizer and mean squared error loss.The code applies the encoder to the drug concentration data to obtain the initial state of the ODE solver.The ODE solver is used to predict drug concentrations over time, and these predictions are fed through the classifier to obtain final predictions. The loss between the predicted drug concentrations and the true drug concentrations is computed and backpropagation is performed to update the model parameters.At the end of each epoch, the code computes the root mean squared error between the model predictions and if the RMSE on the validation set is lower than the current best RMSE, the model parameters are saved as the new best parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "        \n",
    "        train_res = utils.compute_loss_on_test(encoder, ode_func, classifier, args,\n",
    "            tdm1_obj[\"train_dataloader\"], tdm1_obj[\"n_train_batches\"], \n",
    "            device, phase=\"train\")\n",
    "\n",
    "        validation_res = utils.compute_loss_on_test(encoder, ode_func, classifier, args,\n",
    "            tdm1_obj[\"val_dataloader\"], tdm1_obj[\"n_val_batches\"], \n",
    "            device, phase=\"validate\")\n",
    "        \n",
    "        train_loss = train_res[\"loss\"] \n",
    "        validation_loss = validation_res[\"loss\"]\n",
    "        if validation_loss < best_rmse:\n",
    "            torch.save({'encoder': encoder.state_dict(),\n",
    "                        'ode': ode_func.state_dict(),\n",
    "                        'classifier': classifier.state_dict(),\n",
    "                        'args': args}, ckpt_path)\n",
    "            best_rmse = validation_loss\n",
    "            best_epochs = epoch\n",
    "\n",
    "        message = \"\"\"\n",
    "        Epoch {:04d} | Training loss {:.6f} | Training R2 {:.6f} | Validation loss {:.6f} | Validation R2 {:.6f}\n",
    "        Best loss {:.6f} | Best epoch {:04d}\n",
    "        \"\"\".format(epoch, train_loss, train_res[\"r2\"], validation_loss, validation_res[\"r2\"], best_rmse, best_epochs)\n",
    "        logger.info(message)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This portion of the code is used to test the performance of the best performed model. It tests this model on the test set and computes RMSE. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "###############################################################\n",
    "## Main runnings\n",
    "ckpt_path = os.path.join(args.save, f\"fold_{args.fold}_model_{args.model}.ckpt\")\n",
    "eval_path = os.path.join(args.save, f\"fold_{args.fold}_model_{args.model}.csv\")\n",
    "res_path = \"rmse.csv\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code initializes the device for training using either the GPU (if available) or the CPU. It then sets the checkpoint and evaluation file paths, as well as the path for saving the RMSE results.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tdm1_obj = parse_tdm1(device, phase=\"test\")\n",
    "input_dim = tdm1_obj[\"input_dim\"]\n",
    "hidden_dim = 128 \n",
    "latent_dim = 6\n",
    "\n",
    "encoder = Encoder(input_dim=input_dim, output_dim=2 * latent_dim, hidden_dim=hidden_dim)\n",
    "ode_func = ODEFunc(input_dim=latent_dim, hidden_dim=16)\n",
    "classifier = Classifier(latent_dim=latent_dim, output_dim=1)\n",
    "\n",
    "utils.load_model(ckpt_path, encoder, ode_func, classifier, device)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This portion of the code sets up the testing phase by loading the optimal model and recreating the three separate nn models. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Predict & Evaluate\n",
    "with torch.no_grad():\n",
    "    test_res = utils.compute_loss_on_test(encoder, ode_func, classifier, args,\n",
    "        tdm1_obj[\"test_dataloader\"], tdm1_obj[\"n_test_batches\"], \n",
    "        device, phase=\"test\")\n",
    "\n",
    "eval_results = pd.DataFrame(test_res).drop(columns=\"loss\")\n",
    "eval_results.to_csv(eval_path, index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Computes loss on the test set and stores the loss through the .to_csv function. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
